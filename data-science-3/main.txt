from math import sqrt

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as sct
import seaborn as sns
import statsmodels.api as sm
import statsmodels.stats as st
from sklearn.decomposition import PCA

from loguru import logger

# Algumas configurações para o matplotlib.
%matplotlib inline

from IPython.core.pylabtools import figsize


figsize(12, 8)

sns.set()

fifa = pd.read_csv("fifa.csv")

columns_to_drop = ["Unnamed: 0", "ID", "Name", "Photo", "Nationality", "Flag",
                   "Club", "Club Logo", "Value", "Wage", "Special", "Preferred Foot",
                   "International Reputation", "Weak Foot", "Skill Moves", "Work Rate",
                   "Body Type", "Real Face", "Position", "Jersey Number", "Joined",
                   "Loaned From", "Contract Valid Until", "Height", "Weight", "LS",
                   "ST", "RS", "LW", "LF", "CF", "RF", "RW", "LAM", "CAM", "RAM", "LM",
                   "LCM", "CM", "RCM", "RM", "LWB", "LDM", "CDM", "RDM", "RWB", "LB", "LCB",
                   "CB", "RCB", "RB", "Release Clause"
]

try:
    fifa.drop(columns_to_drop, axis=1, inplace=True)
except KeyError:
    logger.warning(f"Columns already dropped")

#Número máximo de colunas apresentadas  
pd.options.display.max_columns = 40

#Dimensão do dataset
fifa.shape

fifa.head()

fifa.describe()

fifa.info()

Geral = pd.DataFrame({'colunas':fifa.columns, 
                      'tipo':fifa.dtypes,
                      'Qtde valores NaN':fifa.isna().sum(),
                      '% valores NaN':fifa.isna().sum()/fifa.shape[0],
                      'valores únicos por feature':fifa.nunique()})
Geral = Geral.reset_index()
Geral

#Como a quantidade de valores faltantes é mínima optei por excluí-los

fifa_drop = fifa.dropna()

fifa_drop.shape

pca = PCA().fit(fifa_drop) #Instanciando o PCA
evr = pca.explained_variance_ratio_
evr

g = plt.bar(range(len(evr)),evr)
plt.xlabel('Numero de compon')
plt.ylabel('Variância explicada');

g = sns.lineplot(np.arange(len(evr)), np.cumsum(evr))
g.axes.axhline(0.95, ls="--", color="red")
plt.xlabel('Numero de componentes')
plt.ylabel('Variância explicada acumulada');

def q1():
    pca = PCA() #Instanciando o PCA (quando não é definido o número de componentes são considerados todos)
    pca.fit_transform(fifa_drop) 
    evr = pca.explained_variance_ratio_
    return round(evr[0],3)

q1()

def q2():
    pca = PCA()
    pca.fit_transform(fifa_drop)
    taxa_de_variancia_acumulada = np.cumsum(pca.explained_variance_ratio_)
    numero_de_features = np.argmax(taxa_de_variancia_acumulada >= 0.95) + 1 # Contagem começa em zero.
    return numero_de_features

q2()

x = [0.87747123,  -1.24990363,  -1.3191255, -36.7341814,
     -35.55091139, -37.29814417, -28.68671182, -30.90902583,
     -42.37100061, -32.17082438, -28.86315326, -22.71193348,
     -38.36945867, -20.61407566, -22.72696734, -25.50360703,
     2.16339005, -27.96657305, -33.46004736,  -5.08943224,
     -30.21994603,   3.68803348, -36.10997302, -30.86899058,
     -22.69827634, -37.95847789, -22.40090313, -30.54859849,
     -26.64827358, -19.28162344, -34.69783578, -34.6614351,
     48.38377664,  47.60840355,  45.76793876,  44.61110193,
     49.28911284
]

pca_2componentes = PCA(n_components=2)
pca_2componentes.fit(fifa_drop) 
np.dot(pca_2componentes.components_,x)

def q3():
    pca_2componentes = PCA(n_components=2)
    pca_2componentes.fit(fifa_drop) 
    pc = np.dot(pca_2componentes.components_,x)
    return (round(pc[0],3),round(pc[1],3)) 

q3()

from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE

def q4():
    X = fifa_drop.drop(columns='Overall')
    y = fifa_drop['Overall']

    lr = LinearRegression()
    rfe = RFE(lr, 5)

    solution = rfe.fit(X,y)
    return X.columns[solution.support_]
